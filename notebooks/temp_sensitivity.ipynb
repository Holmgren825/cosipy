{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "after-sympathy",
   "metadata": {},
   "source": [
    "# Input data and temperature sensitivity\n",
    "\n",
    "A continuation of the simple sensitivity study we did in \n",
    "[Sensitivity studies using COSIPY](sensitivity_study.ipynb) is to investigate how changing the surface temperature, or any other input variable, affects the calculation of the surface mass balance. COSIPY reads the meteorological input data from a **netcdf** file during the run, i.e. during each time step the model reads the meteorological data from the corresponding time step in the input file. The input file can be either \"1D\" for a point simulation or \"2D\" for a distributed simulation. The input file can be based on either observed or modeled data.\n",
    "\n",
    "In order to conduct the temperature sensitivity study we first need to do the following:\n",
    "- Create the input file from observations.\n",
    "- Create copies of the input netcdf with different temperature biases.\n",
    "- Adapt the `NAMELIST` instances to point to the biased files. \n",
    "\n",
    "With this done we can run the simulations. But first let's take a look at the input data for Zhadang glacier, which ships with COSPIY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-channel",
   "metadata": {},
   "source": [
    "**The standard imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to change the cwd for the ipython session, otherwise COSIPY\n",
    "# will look for things in the wrong places.\n",
    "import os\n",
    "# This is not really a good method, if cell is re run we end up in the\n",
    "# wrong directory.\n",
    "os.chdir('./../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosipy.cpkernel.cosipy_core import cosipy_core\n",
    "from cosipy.cpkernel.io import IOClass\n",
    "# cfg gives us the NAMELIST\n",
    "import cfg\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import set_matplotlib_formats\n",
    "# Nice plots\n",
    "set_matplotlib_formats('svg')\n",
    "# Have to tell matplotlib to plot inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-trailer",
   "metadata": {},
   "source": [
    "The NAMELIST already contains the path to the input data, let us open it with `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "cfg.NAMELIST['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of the file\n",
    "cfg.NAMELIST['input_netcdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-louisville",
   "metadata": {},
   "source": [
    "Open the file with xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the namelist for path to data and input netcdf. Have to add\n",
    "# input in between.\n",
    "input_path = cfg.NAMELIST['data_path'] + 'input/' +\\\n",
    "             cfg.NAMELIST['input_netcdf']\n",
    "with xr.open_dataset(input_path) as ds:\n",
    "    ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-lithuania",
   "metadata": {},
   "source": [
    "As you can see, this file contain variables such as the 2-meter temperature (T2), relative humidity at 2 meters (RH2) and the cloud cover (N) for one single point. The time coordinate spans between 2009-01-01 00:00:00 and 2009-01-31 22:00:00 on a hourly resolution.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Question: Can you figure out what the variable G is and what unit it has?</b>\n",
    "</div>\n",
    "<div class=\"alert alert-success\">\n",
    "<details>\n",
    "    <summary><b>Click me for a hint!</b></summary>\n",
    "    Try pressing the document symbol to the right in the output above, this shows some extra information about the variable.\n",
    "    </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-cookie",
   "metadata": {},
   "source": [
    "We can quickly take a look at one of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.T2.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-spotlight",
   "metadata": {},
   "source": [
    "## Creating the input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-skirt",
   "metadata": {},
   "source": [
    "In this case COSIPY comes packaged with the processed data which can directly be used to drive a simulation. In a more realistic scenario however, you probably want drive COSIPY with your own data on another glacier than the Zhadang glacier. The `aws2cosipy` module is provided by COSIPY to aid the processing of .csv-files from weather stations into input files which can be used by COSIPY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the create_1D_input function to process a single point\n",
    "from utilities.aws2cosipy.aws2cosipy import create_1D_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-switch",
   "metadata": {},
   "source": [
    "The `create_1D_input` has five arguments: the csv file to process, the name of the resulting input file, the name of a static file describing the altitude, slope and aspect of the point, and the start and end date.\n",
    "\n",
    "The .csv file is located in the same directory as the input file we looked at earlier. It is called `Zhadang_ERA5_2009_2018.csv`. We can process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the file\n",
    "data_folder = cfg.NAMELIST['data_path']\n",
    "file = data_folder + 'input/Zhadang/Zhadang_ERA5_2009_2018.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static file\n",
    "static_file = data_folder + 'static/Zhadang_static.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_name = data_folder + 'input/Zhadang/Zhadang_ERA5_2009.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time start and end.\n",
    "start_date = '20090101'\n",
    "end_date = '20090131'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function, this takes some time.\n",
    "create_1D_input(file, output_name, static_file, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-resolution",
   "metadata": {},
   "source": [
    "Open the file to see that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(output_name) as ds:\n",
    "    ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-offset",
   "metadata": {},
   "source": [
    "This basically recreated the file that we already had, but feel free to try it on another time period. The .csv-file contains data from January 2000 until early January 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-index",
   "metadata": {},
   "source": [
    "## Creating data for temperature bias experiments\n",
    "\n",
    "A temperature bias experiment is essentially a sensitivity study. It explores the effects of changing the temperature by $n$ degrees over the whole time period, let's say we want to know how the glacier responds to a climate which is  two degrees C warmer. However this approach is quite rough since it doesn't take other changes that might come with a warmer climate into account.\n",
    "\n",
    "Setting this up with COSIPY follows a similar approach as described in the [Sensitivity studies with COSIPY](sensitivity_study.ipynb), with the additional step that we have to create one input file for every experiment. Since reading the .csv-file is rather slow, we will start from the file we already have and create copies from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have the xarray object in memory, hence we can creaty copies\n",
    "ds_up = ds.copy()\n",
    "ds_dn = ds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-advisory",
   "metadata": {},
   "source": [
    "We start with a bias of 2 °C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We basically update the variable with the old variable +/- the bias\n",
    "ds_up['T2'] = ds_up['T2'] + bias\n",
    "ds_dn['T2'] = ds_dn['T2'] - bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-profession",
   "metadata": {},
   "source": [
    "**Plot it to make sure it worked**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ds['T2'].plot(ax=ax, label='Default')\n",
    "ds_up['T2'].plot(ax=ax, label='+2')\n",
    "ds_dn['T2'].plot(ax=ax, label='-2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-hearing",
   "metadata": {},
   "source": [
    "It is not the best looking plot, but it serves its purpose to show that the temperatures are slightly offset from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-texas",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Important!</b> We have to save the new datasets!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the paths\n",
    "fpth_up = data_folder + 'input/Zhadang/Zhadang_ERA5_2009_up.nc'\n",
    "fpth_dn = data_folder + 'input/Zhadang/Zhadang_ERA5_2009_dn.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "ds_up.to_netcdf(fpth_up)\n",
    "ds_dn.to_netcdf(fpth_dn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-aging",
   "metadata": {},
   "source": [
    "## Setting up the experiments\n",
    "\n",
    "We now have the input files for our planned runs. The runs can be set up to run similarly to our previous albedo experiments, but this time we change the `input_netcdf` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NAMELIST['input_netcdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a copy of the namelist so we don't change the original.\n",
    "# Default\n",
    "NAMELIST_def = cfg.NAMELIST.copy()\n",
    "# Up 10%\n",
    "NAMELIST_up = cfg.NAMELIST.copy()\n",
    "# Set it to our new file\n",
    "NAMELIST_up['input_netcdf'] = 'Zhadang/Zhadang_ERA5_2009_up.nc'\n",
    "# Down 10%\n",
    "NAMELIST_dn =cfg.NAMELIST.copy()\n",
    "NAMELIST_dn['input_netcdf'] = 'Zhadang/Zhadang_ERA5_2009_dn.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "IO_def = IOClass(NAMELIST_def) \n",
    "# Down\n",
    "IO_dn = IOClass(NAMELIST_dn)\n",
    "# Up\n",
    "IO_up = IOClass(NAMELIST_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "DATA_def = IO_def.create_data_file();\n",
    "RESULTS_def = IO_def.create_result_file();\n",
    "# Down\n",
    "DATA_dn = IO_dn.create_data_file();\n",
    "RESULTS_dn = IO_dn.create_result_file();\n",
    "# Down\n",
    "DATA_up = IO_up.create_data_file();\n",
    "RESULTS_up = IO_up.create_result_file();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-template",
   "metadata": {},
   "source": [
    "As in the previous sensitivity study we put everything in a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists with our experiments\n",
    "exp_list = [[NAMELIST_def, DATA_def, RESULTS_def, IO_def],\n",
    "            [NAMELIST_dn, DATA_dn, RESULTS_dn, IO_dn],\n",
    "            [NAMELIST_up, DATA_up, RESULTS_up, IO_up]\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-georgia",
   "metadata": {},
   "source": [
    "And lets runs the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are still not doing any evaluation\n",
    "stakes_loc = None\n",
    "df_stakes_data = None\n",
    "stake_names = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in exp_list:\n",
    "    \n",
    "    # We pass the index of our point to cosipy_core, since python is zero\n",
    "    # indexed we have to subtract one.\n",
    "    x = 0\n",
    "    y = 0\n",
    "    # DATA is now exp[1] and the namelist exp[0]\n",
    "    model = cosipy_core(exp[1].isel(lat=y, lon=x), y, x, exp[0],\n",
    "                        stake_names=stake_names,\n",
    "                        stake_data=df_stakes_data)\n",
    "    # The corresponding IO instance is exp[3]\n",
    "    # Create numpy arrays which aggregates all local results\n",
    "    exp[3].create_global_result_arrays()\n",
    "\n",
    "\n",
    "    # Here we are unpacking the results from the model run,\n",
    "    # getting ready to save it to our RESULTS dataframe.\n",
    "    indY, indX, local_restart, RAIN, SNOWFALL, LWin, LWout, H, LE, B,\\\n",
    "    QRR, MB, surfMB, Q, SNOWHEIGHT, TOTALHEIGHT, TS, ALBEDO, NLAYERS,\\\n",
    "    ME, intMB, EVAPORATION, SUBLIMATION, CONDENSATION, DEPOSITION,\\\n",
    "    REFREEZE, subM, Z0, surfM, MOL, LAYER_HEIGHT, LAYER_RHO, LAYER_T,\\\n",
    "    LAYER_LWC, LAYER_CC, LAYER_POROSITY, LAYER_ICE_FRACTION,\\\n",
    "    LAYER_IRREDUCIBLE_WATER, LAYER_REFREEZE, stake_names, stat,df_eval = model\n",
    "    \n",
    "    \n",
    "                   \n",
    "    exp[3].copy_local_to_global(indY, indX, RAIN, SNOWFALL, LWin, LWout, H, LE,\n",
    "                                B, QRR, MB, surfMB, Q, SNOWHEIGHT, TOTALHEIGHT,\n",
    "                                TS, ALBEDO, NLAYERS, ME, intMB, EVAPORATION, \n",
    "                                SUBLIMATION,  CONDENSATION, DEPOSITION,\n",
    "                                REFREEZE, subM, Z0, surfM, MOL, LAYER_HEIGHT,\n",
    "                                LAYER_RHO, LAYER_T, LAYER_LWC, LAYER_CC,\n",
    "                                LAYER_POROSITY, LAYER_ICE_FRACTION,\n",
    "                                LAYER_IRREDUCIBLE_WATER,\n",
    "                                LAYER_REFREEZE)\n",
    "\n",
    "    # Write results to file\n",
    "    exp[3].write_results_to_file()\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-owner",
   "metadata": {},
   "source": [
    "And we can re-use the snippet of code to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Default', '-2$\\degree$C', '+2$\\degree$C']\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for exp, label in zip(exp_list, labels):\n",
    "    # Get the data and plot it RESULTS are kept at the third spot, index 2\n",
    "    exp[2].surfMB.plot(ax=ax, label=label)\n",
    "plt.legend(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-steering",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <details>\n",
    "        <summary>\n",
    "            <b>Question: Do you understand the plot above?</b> <i>Click me for an explanation</i>\n",
    "        </summary>\n",
    "        Similarly to the albedo experiment we did in an earlier notebook changing the temperature directly affects the surface mass balance. Lowering the temperature (negative bias) leads to less mass loss during the day, while increasing the temperature results in increased mass loss.\n",
    "    </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-tyler",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "[Back to overview](welcome.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
