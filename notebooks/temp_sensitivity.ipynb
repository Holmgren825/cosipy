{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-adoption",
   "metadata": {},
   "source": [
    "# Input data and temperature sensitivity\n",
    "\n",
    "A continuation of the simple sensitivity study we did in \n",
    "[Sensitivity studies using COSIPY](sensitivity_study.ipynb) is to investigate how changing the surface temperature, or any other input variable, affects the calculation of the surface mass balance. COSIPY reads the meteorological input data from a **netcdf** file during the run, i.e. during each time step the model reads the meteorological data from the corresponding time step in the input file. The input file can be either \"1D\" for a point simulation or \"2D\" for a distributed simulation. The input file can be based on either observed or modeled data.\n",
    "\n",
    "In order to conduct the temperature sensitivity study we first need to do the following:\n",
    "- Create the input file from observations.\n",
    "- Create copies of the input netcdf with different temperature biases.\n",
    "- Adapt the `NAMELIST` instances to point to the biased files. \n",
    "\n",
    "With this done we can run the simulations. But first let's take a look at the input data for Zhadang glacier, which ships with COSPIY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-mission",
   "metadata": {},
   "source": [
    "**The standard imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to change the cwd for the ipython session, otherwise COSIPY\n",
    "# will look for things in the wrong places.\n",
    "import os\n",
    "# This is not really a good method, if cell is re run we end up in the\n",
    "# wrong directory.\n",
    "os.chdir('./../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosipy.cpkernel.cosipy_core import cosipy_core\n",
    "from cosipy.cpkernel.io import IOClass\n",
    "# cfg gives us the NAMELIST\n",
    "import cfg\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to tell matplotlib to plot inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-tradition",
   "metadata": {},
   "source": [
    "The NAMELIST already contains the path to the input data, let us open it with `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "cfg.NAMELIST['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of the file\n",
    "cfg.NAMELIST['input_netcdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-villa",
   "metadata": {},
   "source": [
    "Open the file with xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the namelist for path to data and input netcdf. Have to add\n",
    "# input in between.\n",
    "input_path = cfg.NAMELIST['data_path'] + 'input/' +\\\n",
    "             cfg.NAMELIST['input_netcdf']\n",
    "with xr.open_dataset(input_path) as ds:\n",
    "    ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-virtue",
   "metadata": {},
   "source": [
    "As you can see, this file contain variables such as the 2-meter temperature (T2), relative humidity at 2 meters (RH2) and the cloud cover (N) for one single point. The time coordinate spans between 2009-01-01 00:00:00 and 2009-01-31 22:00:00 on a hourly resolution.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Question: Can you figure out what the variable G is and what unit it has?</b>\n",
    "</div>\n",
    "<div class=\"alert alert-success\">\n",
    "<details>\n",
    "    <summary><b>Click me for a hint!</b></summary>\n",
    "    Try pressing the document symbol to the right in the output above, this shows some extra information about the variable.\n",
    "    </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-mapping",
   "metadata": {},
   "source": [
    "We can quickly take a look at one of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.T2.plot(figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-thompson",
   "metadata": {},
   "source": [
    "## Creating the input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-queensland",
   "metadata": {},
   "source": [
    "In this case COSIPY comes packaged with the processed data which can directly be used to drive a simulation. In a more realistic scenario however, you probably want drive COSIPY with your own data on another glacier than the Zhadang glacier. The `aws2cosipy` module is provided by COSIPY to aid the processing of .csv-files from weather stations into input files which can be used by COSIPY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the create_1D_input function to process a single point\n",
    "from utilities.aws2cosipy.aws2cosipy import create_1D_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-savannah",
   "metadata": {},
   "source": [
    "The `create_1D_input` has five arguments: the csv file to process, the name of the resulting input file, the name of a static file describing the altitude, slope and aspect of the point, and the start and end date.\n",
    "\n",
    "The .csv file is located in the same directory as the input file we looked at earlier. It is called `Zhadang_ERA5_2009_2018.csv`. We can process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the file\n",
    "data_folder = cfg.NAMELIST['data_path']\n",
    "file = data_folder + 'input/Zhadang/Zhadang_ERA5_2009_2018.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static file\n",
    "static_file = data_folder + 'static/Zhadang_static.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_name = data_folder + 'input/Zhadang/Zhadang_ERA5_2009.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time start and end.\n",
    "start_date = '20090101'\n",
    "end_date = '20090131'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function, this takes some time.\n",
    "create_1D_input(file, output_name, static_file, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-baker",
   "metadata": {},
   "source": [
    "Open the file to see that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(output_name) as ds:\n",
    "    ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-mileage",
   "metadata": {},
   "source": [
    "This basically recreated the file that we already had, but feel free to try it on another time period. The .csv-file contains data from January 2000 until early January 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-leader",
   "metadata": {},
   "source": [
    "## Creating datasets for temperature bias experiments\n",
    "\n",
    "A temperature bias experiment is essentially a sensitivity study. It explores the effects of changing the temperature by $n$ degrees over the whole time period, let's say we want to know how the glacier responds to a climate which is  2 °C warmer. However this approach is quite rough since it doesn't take other changes that might come with a warmer climate into account.\n",
    "\n",
    "Setting this up with COSIPY follows a similar approach as described in the [Sensitivity studies with COSIPY](sensitivity_study.ipynb). However, instead of manipulating the namelist we are changing copies of the dataset containing the input data before sending it to the `cosipy_core`.\n",
    "\n",
    "We begin by initiating the IOClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it is default, we're using cfg.NAMELIST.\n",
    "IO_def = IOClass(cfg.NAMELIST) \n",
    "IO_dn = IOClass(cfg.NAMELIST) \n",
    "IO_up = IOClass(cfg.NAMELIST) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-somalia",
   "metadata": {},
   "source": [
    "Then create the DATA and RESULTS datasets using the IOClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "DATA_def = IO_def.create_data_file();\n",
    "RESULTS_def = IO_def.create_result_file();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-nicholas",
   "metadata": {},
   "source": [
    "And the biased datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down\n",
    "DATA_dn = IO_dn.create_data_file()\n",
    "RESULTS_dn = IO_dn.create_result_file()\n",
    "# Down\n",
    "DATA_up = IO_up.create_data_file()\n",
    "RESULTS_up = IO_up.create_result_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-brave",
   "metadata": {},
   "source": [
    "And then apply the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = 2\n",
    "DATA_dn['T2'] = DATA_dn['T2'] - bias\n",
    "DATA_up['T2'] = DATA_up['T2'] + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-modem",
   "metadata": {},
   "source": [
    "**Plot it to make sure it worked**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "DATA_def['T2'].plot(ax=ax, label='Default')\n",
    "DATA_up['T2'].plot(ax=ax, label='+2')\n",
    "DATA_dn['T2'].plot(ax=ax, label='-2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-richardson",
   "metadata": {},
   "source": [
    "### Setting up the run\n",
    "As in the previous sensitivity study we put everything in a list of lists. Note that we're not changing the namelist this time, only the input data and results dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists with our experiments\n",
    "exp_list = [[cfg.NAMELIST, DATA_def, RESULTS_def, IO_def],\n",
    "            [cfg.NAMELIST, DATA_dn, RESULTS_dn, IO_dn],\n",
    "            [cfg.NAMELIST, DATA_up, RESULTS_up, IO_up]\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-gregory",
   "metadata": {},
   "source": [
    "And lets runs the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are still not doing any evaluation\n",
    "stakes_loc = None\n",
    "df_stakes_data = None\n",
    "stake_names = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in exp_list:\n",
    "    \n",
    "    # We pass the index of our point to cosipy_core, since python is zero\n",
    "    # indexed we have to subtract one.\n",
    "    x = 0\n",
    "    y = 0\n",
    "    # DATA is now exp[1] and the namelist exp[0]\n",
    "    model = cosipy_core(exp[1].isel(lat=y, lon=x), y, x, exp[0],\n",
    "                        stake_names=stake_names,\n",
    "                        stake_data=df_stakes_data)\n",
    "    # The corresponding IO instance is exp[3]\n",
    "    # Create numpy arrays which aggregates all local results\n",
    "    exp[3].create_global_result_arrays()\n",
    "\n",
    "\n",
    "    # Here we are unpacking the results from the model run,\n",
    "    # getting ready to save it to our RESULTS dataframe.\n",
    "    indY, indX, local_restart, RAIN, SNOWFALL, LWin, LWout, H, LE, B,\\\n",
    "    QRR, MB, surfMB, Q, SNOWHEIGHT, TOTALHEIGHT, TS, ALBEDO, NLAYERS,\\\n",
    "    ME, intMB, EVAPORATION, SUBLIMATION, CONDENSATION, DEPOSITION,\\\n",
    "    REFREEZE, subM, Z0, surfM, MOL, LAYER_HEIGHT, LAYER_RHO, LAYER_T,\\\n",
    "    LAYER_LWC, LAYER_CC, LAYER_POROSITY, LAYER_ICE_FRACTION,\\\n",
    "    LAYER_IRREDUCIBLE_WATER, LAYER_REFREEZE, stake_names, stat,df_eval = model\n",
    "    \n",
    "    \n",
    "                   \n",
    "    exp[3].copy_local_to_global(indY, indX, RAIN, SNOWFALL, LWin, LWout, H, LE,\n",
    "                                B, QRR, MB, surfMB, Q, SNOWHEIGHT, TOTALHEIGHT,\n",
    "                                TS, ALBEDO, NLAYERS, ME, intMB, EVAPORATION, \n",
    "                                SUBLIMATION,  CONDENSATION, DEPOSITION,\n",
    "                                REFREEZE, subM, Z0, surfM, MOL, LAYER_HEIGHT,\n",
    "                                LAYER_RHO, LAYER_T, LAYER_LWC, LAYER_CC,\n",
    "                                LAYER_POROSITY, LAYER_ICE_FRACTION,\n",
    "                                LAYER_IRREDUCIBLE_WATER,\n",
    "                                LAYER_REFREEZE)\n",
    "\n",
    "    # Write results to file\n",
    "    exp[3].write_results_to_file()\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-cheat",
   "metadata": {},
   "source": [
    "And we can re-use the snippet of code to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Default', '-2$\\degree$C', '+2$\\degree$C']\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for exp, label in zip(exp_list, labels):\n",
    "    # Get the data and plot it RESULTS are kept at the third spot, index 2\n",
    "    exp[2].surfMB.plot(ax=ax, label=label)\n",
    "plt.legend(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-geometry",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <details>\n",
    "        <summary>\n",
    "            <b>Question: Do you understand the plot above?</b> <i>Click me for an explanation</i>\n",
    "        </summary>\n",
    "        Similarly to the albedo experiment we did in an earlier notebook changing the temperature directly affects the surface mass balance. Lowering the temperature (negative bias) leads to less mass loss during the day, while increasing the temperature results in increased mass loss.\n",
    "    </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-brave",
   "metadata": {},
   "source": [
    "## How is the surface mass balance calculated\n",
    "\n",
    "We have now seen how changing the surface temperature affects the calculation of  surface mass balance on our glacier. To understand this a bit better we will go through how COSIPY is calculating the surface mass balance. Lets start from the top.\n",
    "\n",
    "The surface mass balance, which is measured in m w.e. (meter water equivalent),  is the result of adding the snowfall ($SF$) and deposition ($D$) together and removing the melt ($M$), the sublimation ($s$) and the evaporation/condensation ($e$)\n",
    "\n",
    "$$\n",
    "MB_{surf} = SF + D - M - s - e.\n",
    "$$\n",
    "\n",
    "Note that the evaporation/condensation depends on the sign. We can plot the variables from our `RESULTS` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "mb_vars = ['SNOWFALL', 'surfM', 'SUBLIMATION', 'DEPOSITION', 'EVAPORATION',\n",
    "           'CONDENSATION']\n",
    "for var in mb_vars:\n",
    "    RESULTS_def[var].plot(ax=ax, label=var)\n",
    "ax.set_ylabel('m w.e.')    \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-literature",
   "metadata": {},
   "source": [
    "For our simulated period the plot shows us that sublimation and snowfall make up the main part of the changing mass balance. But there is also a small contribution from deposition. Others are negligible.\n",
    "\n",
    "### Snowfall\n",
    "**Snowfall** can be a direct product of the weather station/gcm data. In this case it is only multiplied by a constant (`mult_factor_RRR`) and then used directly in the surface mass balance calculation. \n",
    "\n",
    "In the case when only the rain rates are provided **snowfall** is derived using a logistic transfer function\n",
    "\n",
    "$$\n",
    "\\mathrm{SNOWFALL} = \\frac{RRR}{1000.0} \\frac{\\rho_{ice}}{\\rho_{fresh\\space snow}} \\frac{-tanh((T2 - t_0) - c_1) \\cdot c_2) + 1}{2}\n",
    "$$\n",
    "\n",
    "where $c_1$ and $c_2$ are the center and spread transfer functions. This smoothly scales the solid precipitation from 100% at $t_0$ and 0% at 2 °C. The density of fresh snow is calculated as a function of the air temperature and wind velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-republic",
   "metadata": {},
   "source": [
    "### Melt\n",
    "\n",
    "The surface melt is calculated from the energy available for melt ($q_m$) and the latent heat required for melting ice ($L_s$)\n",
    "\n",
    "$$\n",
    "\\mathrm{M} = \\frac{q_m \\cdot dt}{L_s \\cdot 1000}.\n",
    "$$\n",
    "\n",
    "We multiply with dt to get the total energy available during the time step, and divide with $1000$ to convert the result to m w.e.\n",
    "\n",
    "The energy available for melt is the sum of the radiative fluxes:\n",
    "- Net short wave radiation\n",
    "- Net long wave radiation\n",
    "- Ground heat flux\n",
    "- Rain heat flux \n",
    "- Sensible heat flux\n",
    "- Latent heat flux\n",
    "\n",
    "These are available in the `RESULTS` as well so we can take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "(RESULTS_def['LWin'] + RESULTS_def['LWout']).plot(ax=ax, label='LW net')\n",
    "# Short wave net is actually not in the Results but we can calculate it\n",
    "(RESULTS_def['G'] * (1 - RESULTS_def['ALBEDO'])).plot(ax=ax, label='SW net')\n",
    "RESULTS_def['H'].plot(ax=ax, label='Sensible heat flux')\n",
    "RESULTS_def['LE'].plot(ax=ax, label='Latent heat flux')\n",
    "RESULTS_def['B'].plot(ax=ax, label='Ground heat flux')\n",
    "RESULTS_def['QRR'].plot(ax=ax, label='Rain heat flux')\n",
    "ax.set_ylabel('Energy [W m$^{-2}$]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-decline",
   "metadata": {},
   "source": [
    "This plot contains a lot of information but there are some key points that you should notice\n",
    "- During the day, the **net short wave** radiation (orange), which act to heat the surface of the glacier is largely compensated by the negative **ground heat flux**. This means that energy is transported into the glacier, slowly heating it.\n",
    "- During the night, the sign of the **ground heat flux** is reversed when the glacier release energy in the form of outgoing **long wave radiation**.\n",
    "\n",
    "From the look of it, the net energy of the variables above should be close to zero. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "RESULTS_def['ME'].plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-fetish",
   "metadata": {},
   "source": [
    "**More to come**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-vampire",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "[Back to overview](welcome.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
