{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seven-planning",
   "metadata": {},
   "source": [
    "# Input data and temperature sensitivity\n",
    "\n",
    "A continuation of the simple sensitivity study we did in \n",
    "[Sensitivity studies using COSIPY](sensitivity_study.ipynb) is to investigate how changing the surface temperature, or any other input variable, affects the calculation of the surface mass balance. COSIPY reads the meteorological input data from a **netcdf** file during the run, i.e. during each time step the model reads the meteorological data from the corresponding time step in the input file. The input file can be either \"1D\" for a point simulation or \"2D\" for a distributed simulation. The input file can be based on either observed or modeled data.\n",
    "\n",
    "In order to conduct the temperature sensitivity study we first need to do the following:\n",
    "- Create the input file from observations.\n",
    "- Create copies of the input netcdf with different temperature biases.\n",
    "- Adapt the `NAMELIST` instances to point to the biased files. \n",
    "\n",
    "With this done we can run the simulations. But first let's take a look at the input data for Zhadang glacier, which ships with COSPIY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-pencil",
   "metadata": {},
   "source": [
    "**The standard imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to change the cwd for the ipython session, otherwise COSIPY\n",
    "# will look for things in the wrong places.\n",
    "import os\n",
    "import sys\n",
    "# This is not really a good method, if cell is re run we end up in the\n",
    "# wrong directory.\n",
    "os.chdir('./../')\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosipy.utils import edu_utils\n",
    "# cfg gives us the NAMELIST\n",
    "import cfg\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to tell matplotlib to plot inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-wealth",
   "metadata": {},
   "source": [
    "The NAMELIST already contains the path to the input data, let us open it with `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "cfg.NAMELIST['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of the file\n",
    "cfg.NAMELIST['input_netcdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-rapid",
   "metadata": {},
   "source": [
    "Open the file with xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the namelist for path to data and input netcdf. Have to add\n",
    "# input in between.\n",
    "input_path = cfg.NAMELIST['data_path'] + 'input/' +\\\n",
    "             cfg.NAMELIST['input_netcdf']\n",
    "with xr.open_dataset(input_path) as ds:\n",
    "    ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-jewelry",
   "metadata": {},
   "source": [
    "As you can see, this file contain variables such as the 2-meter temperature (T2), relative humidity at 2 meters (RH2) and the cloud cover (N) for one single point. The time coordinate spans between 2009-01-01 00:00:00 and 2009-01-31 22:00:00 on a hourly resolution.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<details>\n",
    "    <summary><b>Question: Can you figure out what the variable G is and what unit it has?</b> <i>Click me for a hint!</i></summary>\n",
    "    Try pressing the document symbol to the right in the output above, this shows some extra information about the variable.\n",
    "    </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-framing",
   "metadata": {},
   "source": [
    "We can quickly take a look at one of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.T2.plot(figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-insight",
   "metadata": {},
   "source": [
    "## Creating the input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-geology",
   "metadata": {},
   "source": [
    "In this case COSIPY comes packaged with the processed data which can directly be used to drive a simulation. In a more realistic scenario however, you probably want drive COSIPY with your own data on another glacier than the Zhadang glacier. The `aws2cosipy` module is provided by COSIPY to aid the processing of .csv-files from weather stations into input files which can be used by COSIPY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the create_1D_input function to process a single point\n",
    "from utilities.aws2cosipy.aws2cosipy import create_1D_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-favorite",
   "metadata": {},
   "source": [
    "The `create_1D_input` has five arguments: the csv file to process, the name of the resulting input file, the name of a static file describing the altitude, slope and aspect of the point, and the start and end date.\n",
    "\n",
    "The .csv file is located in the same directory as the input file we looked at earlier. It is called `Zhadang_ERA5_2009_2018.csv`. We can process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the file\n",
    "data_folder = cfg.NAMELIST['data_path']\n",
    "file = data_folder + 'input/Zhadang/Zhadang_ERA5_2009_2018.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static file\n",
    "static_file = data_folder + 'static/Zhadang_static.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_name = data_folder + 'input/Zhadang/Zhadang_ERA5_2009.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time start and end.\n",
    "start_date = '20090101'\n",
    "end_date = '20090131'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function, this takes some time.\n",
    "create_1D_input(file, output_name, static_file, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-punishment",
   "metadata": {},
   "source": [
    "Open the file to see that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(output_name) as ds:\n",
    "    ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-cylinder",
   "metadata": {},
   "source": [
    "This basically recreated the file that we already had, but feel free to try it on another time period. The .csv-file contains data from January 2000 until early January 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-cosmetic",
   "metadata": {},
   "source": [
    "## Creating datasets for temperature bias experiments\n",
    "\n",
    "A temperature bias experiment is essentially a sensitivity study. It explores the effects of changing the temperature by $n$ degrees over the whole time period, let's say we want to know how the glacier responds to a climate which is  2 Â°C warmer. However this approach is quite rough since it doesn't take other changes that might come with a warmer climate into account.\n",
    "\n",
    "Setting this up with COSIPY follows a similar approach as described in the [Sensitivity studies with COSIPY](sensitivity_study.ipynb). However, instead of manipulating the namelist we are changing copies of the dataset containing the input data before sending it to the `cosipy_core`.\n",
    "\n",
    "We begin by initiating the IOClass and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "IO_def, DATA_def, RESULTS_def = edu_utils.create_IO(cfg.NAMELIST)\n",
    "# Up\n",
    "IO_up, DATA_up, RESULTS_up = edu_utils.create_IO(cfg.NAMELIST)\n",
    "# Down\n",
    "IO_dn, DATA_dn, RESULTS_dn = edu_utils.create_IO(cfg.NAMELIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-beverage",
   "metadata": {},
   "source": [
    "And then apply the bias to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias is two degrees\n",
    "bias = 2\n",
    "DATA_dn['T2'] = DATA_dn['T2'] - bias\n",
    "DATA_up['T2'] = DATA_up['T2'] + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-privacy",
   "metadata": {},
   "source": [
    "**Plot it to make sure it worked**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "DATA_def['T2'].plot(ax=ax, label='Default')\n",
    "DATA_up['T2'].plot(ax=ax, label='+2')\n",
    "DATA_dn['T2'].plot(ax=ax, label='-2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-camel",
   "metadata": {},
   "source": [
    "### Setting up the run\n",
    "As in the previous sensitivity study we put everything in a list of lists. Note that we're not changing the namelist this time, only the input data and results dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists with our experiments\n",
    "exp_list = [[cfg.NAMELIST, DATA_def, RESULTS_def, IO_def],\n",
    "            [cfg.NAMELIST, DATA_dn, RESULTS_dn, IO_dn],\n",
    "            [cfg.NAMELIST, DATA_up, RESULTS_up, IO_up]\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-mirror",
   "metadata": {},
   "source": [
    "And lets runs the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in exp_list:\n",
    "    # Call run_model once for each experiment\n",
    "    edu_utils.run_model(exp[1], exp[3], exp[0], exp[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-knock",
   "metadata": {},
   "source": [
    "And we can re-use the snippet of code to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Default', '-2$\\degree$C', '+2$\\degree$C']\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for exp, label in zip(exp_list, labels):\n",
    "    # Get the data and plot it RESULTS are kept at the third spot, index 2\n",
    "    exp[2].surfMB.plot(ax=ax, label=label)\n",
    "plt.legend(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-motorcycle",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <details>\n",
    "        <summary>\n",
    "            <b>Question: Do you understand the plot above?</b> <i>Click me for an explanation</i>\n",
    "        </summary>\n",
    "        Similarly to the albedo experiment we did in an earlier notebook changing the temperature directly affects the surface mass balance. Lowering the temperature (negative bias) leads to less mass loss during the day, while increasing the temperature results in increased mass loss.\n",
    "    </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-circulation",
   "metadata": {},
   "source": [
    "## How the surface mass balance is calculated\n",
    "\n",
    "We have now seen how changing the surface temperature affects the calculation of  surface mass balance on our glacier. To understand this a bit better we will go through how COSIPY is calculating the surface mass balance. Lets start from the top.\n",
    "\n",
    "The surface mass balance, which is measured in m w.e. (meter water equivalent),  is the result of adding the snowfall ($SF$) and deposition ($D$) together and removing the melt ($M$), the sublimation ($s$) and the evaporation/condensation ($e$)\n",
    "\n",
    "$$\n",
    "MB_{surf} = SF + D - M - s - e.\n",
    "$$\n",
    "\n",
    "Note that the evaporation/condensation depends on the sign. We can plot the variables from our `RESULTS` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "mb_vars = ['SNOWFALL', 'surfM', 'SUBLIMATION', 'DEPOSITION', 'EVAPORATION',\n",
    "           'CONDENSATION']\n",
    "for var in mb_vars:\n",
    "    RESULTS_def[var].plot(ax=ax, label=var)\n",
    "ax.set_ylabel('m w.e.')    \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-account",
   "metadata": {},
   "source": [
    "For our simulated period the plot shows us that sublimation and snowfall make up the main part of the changing mass balance. But there is also a small contribution from deposition. Others are negligible.\n",
    "\n",
    "### Snowfall\n",
    "**Snowfall** can be a direct product of the weather station/gcm data. In this case it is only multiplied by a constant (`mult_factor_RRR`) and then used directly in the surface mass balance calculation. \n",
    "\n",
    "In the case when only the rain rates are provided **snowfall** is derived using a logistic transfer function\n",
    "\n",
    "$$\n",
    "\\mathrm{SNOWFALL} = \\frac{RRR}{1000.0} \\frac{\\rho_{ice}}{\\rho_{fresh\\space snow}} \\frac{-tanh((T2 - t_0) - c_1) \\cdot c_2) + 1}{2}\n",
    "$$\n",
    "\n",
    "where $c_1$ and $c_2$ are the center and spread transfer functions. This smoothly scales the solid precipitation from 100% at $t_0$ and 0% at 2 Â°C. The density of fresh snow is calculated as a function of the air temperature and wind velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-hardware",
   "metadata": {},
   "source": [
    "### Melt\n",
    "\n",
    "The surface melt is calculated from the energy available for melt ($q_m$) and the latent heat required for melting ice ($L_s$)\n",
    "\n",
    "$$\n",
    "\\mathrm{M} = \\frac{q_m \\cdot dt}{L_s \\cdot 1000}.\n",
    "$$\n",
    "\n",
    "We multiply with dt to get the total energy available during the time step, and divide with $1000$ to convert the result to m w.e.\n",
    "\n",
    "The energy available for melt is the sum of the radiative fluxes:\n",
    "- Net short wave radiation\n",
    "- Net long wave radiation\n",
    "- Ground heat flux\n",
    "- Rain heat flux \n",
    "- Sensible heat flux\n",
    "- Latent heat flux\n",
    "\n",
    "These are available in the `RESULTS` as well so we can take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "(RESULTS_def['LWin'] + RESULTS_def['LWout']).plot(ax=ax, label='LW net')\n",
    "# Short wave net is actually not in the Results but we can calculate it\n",
    "(RESULTS_def['G'] * (1 - RESULTS_def['ALBEDO'])).plot(ax=ax, label='SW net')\n",
    "RESULTS_def['H'].plot(ax=ax, label='Sensible heat flux')\n",
    "RESULTS_def['LE'].plot(ax=ax, label='Latent heat flux')\n",
    "RESULTS_def['B'].plot(ax=ax, label='Ground heat flux')\n",
    "RESULTS_def['QRR'].plot(ax=ax, label='Rain heat flux')\n",
    "ax.set_ylabel('Energy [W m$^{-2}$]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-bundle",
   "metadata": {},
   "source": [
    "This plot contains a lot of information but there are some key points that you should notice\n",
    "- During the day, the **net short wave** radiation (orange), which act to heat the surface of the glacier is largely compensated by the negative **ground heat flux**. This means that energy is transported into the glacier, slowly heating it.\n",
    "- During the night, the sign of the **ground heat flux** is reversed when the glacier release energy in the form of outgoing **long wave radiation**.\n",
    "\n",
    "From the look of it, the net energy of the variables above should be close to zero. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "RESULTS_def['ME'].plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-negotiation",
   "metadata": {},
   "source": [
    "With this in mind we can dissect the radiative fluxes. \n",
    "\n",
    "#### Net shortwave radiation and albedo\n",
    "The **net shortwave radiation** is defined as\n",
    "$$\n",
    "q_{sw} = (1-\\alpha) \\cdot q_G\n",
    "$$\n",
    "\n",
    "where $q_G$ is the incoming shortwave radiation and $\\alpha$ is the **albedo**. $q_G$ is given by the input data, while the evolution of the albedo is parametrized according to the approach from Oerlemans and Knap ([1998](https://www.cambridge.org/core/journals/journal-of-glaciology/article/1-year-record-of-global-radiation-and-albedo-in-the-ablation-zone-of-morteratschgletscher-switzerland/F30150EB1AD9D62A0C007C22FFAE7B6A)). In it, the snow albedo is based on the time since the last snowfall ($s$) and an albedo timescale ($\\tau^*$) that specifies the speed of the degradation of the albedo from that fresh snow ($\\alpha_s$) to that of firn ($\\alpha_f$)\n",
    "\n",
    "$$\n",
    "\\alpha_{snow} = \\alpha_f + (\\alpha_s - \\alpha_f)\\space exp \\left(\\frac{s}{\\tau^*}\\right).\n",
    "$$\n",
    "\n",
    "As the thickness of the snowpack ($d$) decreases, the surface albedo should approach that of the albedo of ice ($\\alpha_i$). This is done by introducing a characteristic snow depth scale ($d^*$) to employ what is known as e-folding. With this, the full surface albedo can be written as \n",
    "\n",
    "$$\n",
    "\\alpha = \\alpha_s + (\\alpha_i - \\alpha_s)\\space exp \\left(\\frac{-d}{d^*}\\right).\n",
    "$$\n",
    "\n",
    "The surface albedo is reset to that of fresh snow when new snow accumulation exceeds a certain threshold.\n",
    "\n",
    "The fraction of the net shortwave radiation that penetrates the glacier decays with the depth ($z$) from the surface according to\n",
    "\n",
    "$$\n",
    "Q(z, t) = \\lambda_r q_{sw} \\space exp(-z \\beta)\n",
    "$$\n",
    "\n",
    "where $\\lambda_r$ is the fraction of absorbed radiation and $\\beta$ the extinction coefficient (both depending on if the integrated depth is snow or ice). \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "   The constants in the equation above are all available in the NAMELIST. Remember that we changed one of them, the albedo of fresh snow, in the sensitivity study. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-shelter",
   "metadata": {},
   "source": [
    "#### Net longwave radiation\n",
    "\n",
    "The **net longwave radiation** can be handled in two different ways:\n",
    "If the input data contains the incoming longwave radiation, the net is calculated as\n",
    "\n",
    "$$\n",
    "q_{lw} = q_{lw_{in}} - \\epsilon_s \\sigma T_0^4,\n",
    "$$\n",
    "\n",
    "where $\\epsilon_s$ is the surface emissivity, in our case constant $\\sim 1$.\n",
    "\n",
    "If the input data doesn't contain the incoming longwave radiation directly, it  will be parametrized using the Stefan-Boltzman law with the means of the air temperature ($T_{zt}$) and the atmospheric emissivity\n",
    "\n",
    "$$\n",
    "\\epsilon_a = \\epsilon_{cs}(1-N^2) + \\epsilon_{cl}N^2,\n",
    "$$\n",
    "\n",
    "where $N$ is the cloud cover fraction and $\\epsilon_{cs}$ and $\\epsilon_{cl}$ are the clear-sky and cloud emissivity. The cloud emissivity is kept constant while the clear-sky emissivity is given by\n",
    "\n",
    "$$\\epsilon_{cs} = 0.23 + 0.433(e_{zt}/T_{zt})^{1/8}.$$\n",
    "\n",
    "where $e_{zt}$ is the water vapor pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-playback",
   "metadata": {},
   "source": [
    "### Sublimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-mountain",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "[Back to overview](welcome.ipynb)\n",
    "\n",
    "[Distributed simulations](distributed_run.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
